package uk.ac.ed.inf.nmr.mapreduce;

import java.io.IOException;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Writable;

import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;

import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import uk.ac.ed.inf.nmr.readers.CommonReaderUtils;
import uk.ac.ed.inf.nmr.readers.ReaderFactory;
import uk.ac.ed.inf.nmr.writers.CommonWriterUtils;
import uk.ac.ed.inf.nmr.writers.WriterFactory;

/**
 * Class extending the {@link Mapper} class in Hadoop and enabling it to start a nested job.
 * 
 * @author Daniel Stanoescu
 *
 * @param <KEYIN> the type of the keys that get generated by the RecordReader from an input split.
 * @param <VALUEIN> the type of the values that get generated by the RecordReader from an input split.
 * @param <KEYOUT> the type of the keys that will exit the NestedMapper function.
 * @param <VALUEOUT> the type of the values that will exit the NestedMapper function.
 */

public class NestedMapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> extends Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {

	/** 
	 * The configuration of the innner job.
	 */
	private Configuration conf = new Configuration();
	
	/**
	 * The job that will become a nestedJob.
	 */
	private Job nestedJob;
	
	/**
	 * Path to the intermediary file that will be used as input in the inner job.
	 */
	private Path nestedJobInputPath;
	
	/**
	 * Path where the nested job will place its output.
	 */
	private Path nestedJobOutputPath;
	/**
	 * Path to the output directory of the outer job.
	 */
	private Path innerWorks;
	
	/**
	 * The type of writer used to create the intermediary files.
	 */
	private String writerType = null;
	
	/**
	 * The type of reader used to read the output of the nested job.
	 */
	private String readerType = "SequenceFile";					// default Nested Job output reader		
	
	/**
	 * The object encapsulating the condition to launch a nested job.
	 */
	public JobTrigger jobTrigger = new JobTrigger();
	
	private String nestedLevel = null;
	
	/* Create a file writer that will write the intermediate key/value pairs */
	WriterFactory writerFactory = new WriterFactory();
	public CommonWriterUtils writer;
	
	/*Create a file reader that will read the output of the nested job*/
	ReaderFactory readerFactory = new ReaderFactory();
	protected CommonReaderUtils reader;
	
	/**
	 * Custom implementation of the Run function in the default Hadoop {@link Mapper}.
	 */
	@Override
	public void run(Context context) throws IOException, InterruptedException{
		
		context.getWorkingDirectory();
		
		setup(context);
			try {
				setupWorkflow(context);
			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InstantiationException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (IllegalAccessException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
		cleanup(context);
	}

	/**
	 * This class defines the internal workflow of the NestedMapper it uses the context from the default {@link Mapper}.
	 * 
	 * @param context Uses the {@link Mapper.Context} of default Hadoop Mapper.
	 * @throws IOException Returned if there are I/O problems with the intermediary structures.
	 * @throws InterruptedException Was intrerupted during an I/O operation with the intermediate files.
	 * @throws ClassNotFoundException Could not locate writer or reader classes.
	 * @throws InstantiationException Could not instantiate writers or readers.
	 * @throws IllegalAccessException Accessed HDFS resources that were not available.
	 */
	
	private void setupWorkflow(Context context) throws IOException, InterruptedException, ClassNotFoundException, InstantiationException, IllegalAccessException {
		
		/* saves the path of the outer job output */
		innerWorks = FileOutputFormat.getOutputPath(context).getParent();
		
		/* creates an inner job job configuration.*/
		nestedJob = new Job(conf);
		
		/* picks up the settings from the user provided configuration */
		setupNestedJob(nestedJob, conf, jobTrigger);

		/* based on the job configuration provided by the user we setup writer and readers */
		setupInternalWR(context);
		
		nestedLevel = nestedJob.getJobName();	

		/* run the nestedMapper */
		setupNestedMap(context);
			
		/* pick up further configuration information after processing the split  */
		setupNestedJob(nestedJob,conf, jobTrigger);
		
		//setupNesting(nestedJob, conf, jobTrigger);
		
		/* execut the nested job */
		while(executeNestedJob(context) != true){
			// busy wait until nesting is completed
		}
		
		try {
			/* run the normal map function*/
			setupNormalMap(context);
		} catch (Exception e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
		
	}
	
	
	/**
	 * Picks up the user provided configuration specifications or uses default ones
	 * @param job Requires a {@link Job} object that will become the nested job.
	 * @param conf Requires a {@link Configuration} of the soon to be nested job.
	 * @param condition Requires a {@link JobTrigger} to control the type of nested job to be created.
	 * @throws IOException -
	 */
	
	protected void setupNestedJob(Job job, Configuration conf, JobTrigger condition) throws IOException{
		setupNesting(nestedJob, conf, condition);
	}
	
	/**
	 * Picks up the user provided configuration specifications or uses default ones
	 * @param job Requires a {@link Job} object that will become the nested job.
	 * @param conf Requires the {@link Configuration} of the soon to be nested job.
	 * @param condition Requires a {@link JobTrigger} to control the type of nested job to be created.
	 * @throws IOException -
	 */
	
	protected void setupNesting(Job job, Configuration conf, JobTrigger condition ) throws IOException{
		// User provided details for nested job configuration
	}
	
	
	/**
	 * Configures the internal write/read artifacts responsible for enabling Nested MapReduce.
	 * @param context The {@link Context} of the default Hadoop {@link Mapper}
	 * @throws IOException
	 * @throws ClassNotFoundException
	 * @throws InstantiationException
	 * @throws IllegalAccessException
	 * @throws InterruptedException
	 */
	
	private void setupInternalWR(Context context)
		throws IOException, ClassNotFoundException, InstantiationException,
							IllegalAccessException, InterruptedException{

		nestedJobInputPath = new Path(innerWorks + "/inceptions/" + nestedJob.getJobName() + "/" + context.getTaskAttemptID().toString());

		if(SequenceFileInputFormat.class.getName() == nestedJob.getInputFormatClass().getName()){
    		SequenceFileInputFormat.addInputPath(nestedJob, nestedJobInputPath);
    		writerType = "SequenceFile";
		}
    				
		if(TextInputFormat.class.getName() == nestedJob.getInputFormatClass().getName()){
			FileInputFormat.addInputPath(nestedJob, nestedJobInputPath);
			writerType = "BufferFile";
		}
		
		if(jobTrigger.getCondition().equals("default")){
			if (TextInputFormat.class.getName() == nestedJob.getInputFormatClass().getName()){
				readerType = "BufferFile";
			}
		}
	}
	
	/**
	 * Configures the nestedMap user provided function and writes it's results to an intermediate file.
	 * @param context The default {@link Context} of the Hadoop {@link Mapper}.
	 * @throws IOException
	 * @throws InterruptedException
	 * @throws ClassNotFoundException
	 * @throws InstantiationException
	 * @throws IllegalAccessException
	 */
	
	protected void setupNestedMap(Context context) throws IOException, InterruptedException, ClassNotFoundException, InstantiationException, IllegalAccessException{

		try {
			writer = writerFactory.makeWriter(context, innerWorks, nestedLevel, writerType);
		} catch (Exception e) {
			e.printStackTrace();
		}
		  
		  while(context.nextKeyValue()){
			  nestedMap(context.getCurrentKey(), context.getCurrentValue(), jobTrigger);
		  }
		  
	    writer.close();
	}
	
	/**
	 * 
	 * @param key Gets a key generated by the {@link NestedMapper} for processing.
	 * @param value Gets a value generated by the {@link NestedMapper} for processing.
	 * @param condition Allows the user to specify a {@link JobTrigger} to determine if a nested job will start or not.
	 * @throws IOException
	 * @throws InterruptedException
	 */
	
	protected void nestedMap (KEYIN key, VALUEIN value, JobTrigger condition) throws IOException, InterruptedException{
		// identity nestedMap
		
		writer.write(key, value);
	}
	
	/**
	 * Based on the status of the {@link JobTrigger} it will either start a nested job or not.
	 * @param context
	 * @return True when the nested job si compelted.
	 * @throws ClassNotFoundException
	 */
	protected boolean executeNestedJob(Context context) throws ClassNotFoundException{
		
		// TODO - maybe change condition to something else (ex: anything but null)
		if (jobTrigger.getCondition() != "default"){	

			nestedJobOutputPath = new Path(innerWorks + "/outputs/" + nestedJob.getJobName() + "/" + context.getTaskAttemptID());

			if(SequenceFileOutputFormat.class.getName() == nestedJob.getOutputFormatClass().getName()){
				SequenceFileOutputFormat.setOutputPath(nestedJob, nestedJobOutputPath);
				readerType = "SequenceFile";
			}

			if(TextOutputFormat.class.getName() == nestedJob.getOutputFormatClass().getName()){
				FileOutputFormat.setOutputPath(nestedJob, nestedJobOutputPath);
				readerType = "BufferFile";
			}

			try{
				nestedJob.waitForCompletion(true);
			} catch (IOException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (InterruptedException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			} catch (ClassNotFoundException e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
		}
		return true;
	}
	
	/**
	 * Sets up the normalMap function and feeds it the output of the nested job.
	 * @param context Uses the {@link Context} of the {@link Mapper} to write the key/value pairs to disk.
	 * @throws Exception
	 */
	private void setupNormalMap(Context context) throws Exception {
		
		if (jobTrigger.getCondition() != "default"){
		
			FileSystem fs = FileSystem.get(conf);
			Path dir = new Path(innerWorks + "/outputs/" + nestedJob.getJobName() + "/" + context.getTaskAttemptID().toString() + "/");
			FileStatus[]stats = fs.listStatus(dir);
			String checker = innerWorks + "/outputs/" + nestedJob.getJobName() + "/" + context.getTaskAttemptID().toString() + "/" + "part-r-";
			
			for(FileStatus stat : stats){
				if(stat.getPath().toUri().getPath().toString().contains(checker)){
				
					reader = readerFactory.makeReader(context, stat.getPath().toUri().getPath().toString(), readerType, jobTrigger.getCondition());
					
					while(reader.next()){
						map(reader.getKey(), reader.getValue(), context);
					}	
				}
			}
		}	
		else{

			reader = readerFactory.makeReader(context, innerWorks, nestedJob.getJobName(), readerType, jobTrigger.getCondition());
			
			while(reader.next()){
				map(reader.getKey(), reader.getValue(), context);
			}
			
		}
	}
	
	/**
	 * Simulates the {@link Mapper} map function and uses key values generated from the nested job.
	 * @param key The key from the output of the nested job.
	 * @param value The value from the output of the nested job.
	 * @param context The {@link Context} of the {@link Mapper} to serialize the final key/value pairs of the {@link NestedMapper}
	 * @throws IOException
	 * @throws InterruptedException
	 */
	@SuppressWarnings("unchecked")
	protected void map(Writable key, Writable value, Context context) throws IOException, InterruptedException{
		// identity mapper
		context.write((KEYOUT) key, (VALUEOUT) value);
	}
	
	
	

}
